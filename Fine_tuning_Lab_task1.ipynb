{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlPzb78drMTmOqOABWDdBW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paul-NIROB/agentic-ai/blob/main/Fine_tuning_Lab_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate evaluate\n"
      ],
      "metadata": {
        "id": "EN4Gq_LMOC7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QquDoYFx_Zst"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "a26_Hi6yJsq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"ag_news\")\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "gRmFGTvFKYUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=4\n",
        ")\n"
      ],
      "metadata": {
        "id": "dgb-jT0lK5R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "UJldlooELGgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "WMP24hFGLXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "mj2y6RtfPH0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "TU0x-khLPP-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n",
        "small_test = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "trainer.train_dataset = small_train\n",
        "trainer.eval_dataset = small_test\n"
      ],
      "metadata": {
        "id": "i-iSJ_GGV1TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "XNb-nnGjV6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "qX_o8SeZnqAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning a Small Language Model (SLM)\n",
        "\n",
        "## Agentic AI â€“ Lab Task 1\n",
        "\n",
        "### Objective\n",
        "The objective of this task is to fine-tune a Small Language Model (SLM) with fewer than 3 billion parameters using a text dataset from Hugging Face and evaluate its performance.\n",
        "\n",
        "### Tools Used\n",
        "- Google Colab\n",
        "- Hugging Face Transformers\n",
        "- Hugging Face Datasets\n",
        "- DistilBERT model\n"
      ],
      "metadata": {
        "id": "RVtmoEguAKdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Description\n",
        "\n",
        "The AG News dataset is a text classification dataset consisting of news articles categorized into four classes:\n",
        "- World\n",
        "- Sports\n",
        "- Business\n",
        "- Science/Technology\n",
        "\n",
        "The dataset is divided into training and testing splits and is suitable for text classification tasks.\n"
      ],
      "metadata": {
        "id": "MJWiLnh1Kjka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Observations\n",
        "\n",
        "The fine-tuned DistilBERT model achieved strong performance on the AG News dataset\n",
        "with high classification accuracy. The training loss decreased steadily, showing\n",
        "effective learning during fine-tuning.\n",
        "\n",
        "Using a reduced dataset significantly decreased training time while maintaining\n",
        "acceptable performance, making this approach suitable for resource-constrained\n",
        "environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "s_QBc-ayoeEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This lab demonstrated the fine-tuning of a Small Language Model with fewer than\n",
        "3 billion parameters using a Hugging Face dataset. The results show that SLMs can\n",
        "be efficiently adapted to downstream tasks using limited computational resources.\n"
      ],
      "metadata": {
        "id": "Fvl4Uih7ol46"
      }
    }
  ]
}